---
title: "Impact of Different Reading Strategies on Text Comprehension"
author: "Hayden Choi"
format: 
    html:
        code-fold: true
        code-summary: "Show code"
        toc: true
        theme: cosmo
        output-file: index.html
execute:
    echo: true
    warning: false
    message: false
    fig-align: center
---

## Introduction

This project aims to investigate the impact of different reading strategies — **highlighting, underlining, and reading without annotations** — on reading comprehension. Using a **within-subjects** experimental design, participants engage with standardized reading passages and complete comprehension tests under each condition. The study also explores **the relationship between the amount of text highlighted/underlined and comprehension scores**.

### Objectives

-   To determine if highlighting or underlining improves reading comprehension compared to reading without annotations.
-   To investigate any potential relationship between the amount of text highlighted/underlined and comprehension scores.

### Hypothesis

**Null Hypothesis (**$H_O$): There is no significant difference in reading comprehension scores across the three strategies (highlighting, underlining, and reading without annotations).

**Alternative Hypothesis (**$H_A$): There is a significant difference in reading comprehension scores across the three strategies, with at least one strategy leading to higher scores.

### Experimental Design

1.  Participants read three passages from practice SAT exams each with a different strategy, in a randomized order. After reading each passage, they solve the corresponding questions. Each passage has a time limit of 13 minutes to be read and solved.
2.  The effects of reading strategies are analyzed using ANOVA on the obtained scores. If there is a significant difference, post-hoc tests are conducted to identify which strategies differ.
3.  Different assumptions for ANOVA are checked, including normality, homogeneity of variance and sphericity.
4.  The proportion of text highlighted/underlined is extracted by analyzing pdfs of the passages that were annotated by participants.
5.  The relationship between the proportion of text highlighted/underlined and comprehension scores is analyzed using linear regression.

Overall, this study aims to contribute to a deeper understanding of effective reading strategies and their implications for study habits and learning outcomes.

## Initial Setup

The first step is to load the required packages and read the data file containing participant responses. We then inspect the first few rows of the dataset to understand its structure.

```{r}
# Load required packages
library(readxl)
library(tidyverse)
library(rstatix)
library(ggpubr)
library(car)
library(knitr)
library(kableExtra)
library(afex)
library(emmeans)
library(broom)
```

```{r}
# Read the data file containing participant responses and inspect the first few rows
df <- read_excel("./data/participant_responses.xls")
kable(head(df), align = "c", caption = "First Six Rows of the Dataframe")
```

## Data Validation and Preprocessing

The next step is to validate the data and perform any necessary preprocessing steps to ensure that the data is clean and ready for analysis.

We first check the data types of each column in the dataframe. This will help us identify any columns that need to be converted to a different data type for analysis.

```{r}
# Check the data types of each column in the dataframe
kable(sapply(df, class), col.names = c("Column Name", "Column Data Type"), align = "c",
      caption = "Data Type of Each Feature")
```

Since the categorical variable `gender` is currently stored as a character vector, we convert it to a factor with appropriate labels for better interpretability.

```{r}
# Turn the categorical variable "gender" into a factor
df$gender <- factor(df$gender, levels = c("f", "m"), labels = c("Female", "Male"))
# Verify whether the gender variable has been successfully transformed into a factor
cat("The 'gender' column type has been successfully converted to a factor:", is.factor(df$gender))
```

We also check for any missing values in the dataframe, as these can impact the results of our analysis.

```{r}
# Check for any missing values in the dataframe
missing_values <- df %>%
  summarise_all(~sum(is.na(.)))
kable(missing_values, align = "c", caption = "Number of Missing Values in Each Column")
```

There are no missing values in the dataframe, so we can proceed with the analysis without needing to handle any missing data.

Before moving on to the actual analysis, we stack the data to convert it from wide to long format. This will make it easier to analyze and visualize the data.

```{r}
# Stack the data to convert it from wide to long format for easier analysis and visualization
df_long <- df %>%
  select(-gender, -age) %>%
  pivot_longer(
    cols = c(passive_score, underline_score, highlight_score),
    names_to = "strategy",
    values_to = "score") %>%
  arrange(strategy, id)
df_long$strategy <- as.factor(df_long$strategy)
df_long$id <- as.factor(df_long$id)
# Verify the first few rows of the stacked dataframe
kable(head(df_long), align = "c", caption = "First Six Rows of the Stacked Dataframe")
```

## Exploratory Data Analysis

We now perform exploratory data analysis to understand the distribution of participant characteristics and reading comprehension scores across different strategies.

```{r}
# Show the overall summary of statistics for the dataframe
kable(summary(df), align = "c", caption = "Summary Statistics of the Dataframe")
```

```{r}
# Visualize the distribution of the age of participants
ggplot(df, aes(x = age)) +
  geom_histogram(binwidth = 1, fill = "darkseagreen", alpha = 0.5, colour="darkseagreen") + 
  labs(title = "Distribution of Age", x = "Age", y = "Count") + 
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

As the experiment was conducted on high school students preparing for the SAT test, the age range is relatively narrow with most participants being 16-17 years old.

```{r}
# Visualize the distribution of the gender of participants
ggplot(df, aes(x = gender)) +
  geom_bar(aes(colour = gender, fill = gender), alpha = 0.5) + 
  labs(title = "Distribution of Gender", x = "Gender", y = "Count") + 
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

As both boys and girls have been roughly equally represented in this experiment, this ensures that the results are not heavily impacted by gender.

```{r}
# Visualize the distribution of reading comprehension scores by strategy as boxplots
ggplot(df_long, aes(x = strategy, y = score)) +
  geom_boxplot(colour = "darkseagreen", outlier.color = "red") + 
  geom_jitter(colour = "darkseagreen", alpha = 0.5, size = 2, width = 0.4, height = 0) + 
  labs(title = "Boxplot of Score Distributions by Reading Strategy", x = "Reading Strategy", y = "Score (%)", 
       caption = "Any outliers are represented by red points"
  ) + 
  ylim(0, 100) + 
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# Visualize the distribution of reading comprehension scores by strategy as density plots
ggplot(df_long, aes(x = score, fill = strategy, colour = strategy)) + 
  geom_density(alpha = 0.5) +
  labs(title = "Density Plot of Score Distributions by Reading Strategy",
       x = "Test Score (%)", y = "Density") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

Looking at the distribution plots, it can be noted that there might be potential differences in the distributions of scores across strategies. To test whether these differences are statistically significant, we will conduct a repeated measures ANOVA test.

## Validation of Assumptions

Before conducting the ANOVA test, we need to check the assumptions that are required to make the results of the test valid. These assumptions include: absence of outliers, normality, homogeneity of variance and sphericity.

### Absence of Outliers

```{r}
# Check for outliers in the data
outlier <- df_long %>%
  group_by(strategy) %>%
  identify_outliers(score)
cat("Number of outlier points in the data:", nrow(outlier))
```

This result, combined with the boxplot above, shows that there are no outliers in the data.

### Normality

```{r}
# Check the normality using the Shapiro-Wilk test
normality <- df_long %>%
  group_by(strategy) %>%
  shapiro_test(score)
kable(normality, align = "c", caption = "Results of the Shapiro-Wilk Test for Normality of Data")
```

The p-values for strategies are all greater than 0.05. This indicates that we fail to reject the null hypothesis of normality.

### Homogeneity of Variance

```{r}
# Use Bartlett's test to check for homogeneity of variance
bartlett.test(score ~ strategy, data=df_long)
```

As the p-value is greater than 0.05, we fail to reject the null hypothesis of homogeneity of variance. This indicates that the variances of the scores across groups are equal.

### Sphericity

In a repeated measures ANOVA test, it is assumed that the variances of differences between all combinations of related condition groups are equal. If this assumption is violated, there would be an increase in the Type I error rate due to distorted variance calculations. In this case, the ANOVA results must be appropriately adjusted according to the degree of which sphericity has been violated.

In this report, we first manually inspect the variances of the differences between each combination of reading strategies.

```{r}
# Calculate the raw difference between the test scores of each pair of reading strategies
grp.diff <- df %>% 
  transmute(
    'passive_vs_underline' = passive_score - underline_score, 
    'passive_vs_highlight' = passive_score - highlight_score, 
    'highlight_vs_underline' = highlight_score - underline_score
  )

# Compute the variance of the differences between each pair of reading strategies
grp.diff %>%
  map(var) %>%
  as.data.frame() %>% 
  t() %>%
  kable(
    col.names = c("Strategies Under Comparison", "Variance"),
    align = "c",
    caption = "Variance of Differences Between Reading Strategy Groups"
  )
```

The variance of "passive - underline" group is smaller by a large margin compared to that of the other two groups, suggesting a potential violation of the sphericity assumption. To validate this, we can use the Mauchly's test of sphericity, which is automatically included in the ANOVA test function.

## ANOVA Test

We will now conduct a one-way repeated measures ANOVA test to determine if there are any significant differences in reading comprehension scores across the three strategies.

```{r}
# Conduct a repeated measures ANOVA test with inherent corrections for the potential violation of sphericity
aov_rm <- aov_ez(data=df_long, dv="score", id="id", within="strategy")
summary(aov_rm)
```

At first glance, it seems like the difference is statistically significant, as the p-value shown at the table in the top of the output is less than 0.05. However, the results from the Mauchly's test indicate that we reject the null hypothesis of sphericity across the conditions, as the p-value is less than 0.05. In order to correct for this, the Greenhouse-Geisser correction ($\epsilon$ = 0.53) is applied to the degrees of freedom in the table at the bottom of the output. The adjusted p-value is still less than 0.05, indicating that there is a significant difference in the scoress across the three different reading strategies.

## Post-hoc Tests

To identify which specific strategies differ from each other, we will conduct post-hoc tests using the estimated marginal means (EMM) method. This method is appropriate for repeated measures designs and allows us to control for family-wise error rates.

```{r}
# Compute the estimated marginal means for each condition
emm <- emmeans(aov_rm, ~ strategy)
# Pairwise comparisons with Holm family-wise error correction
post <- pairs(emm, adjust = "holm")
kable(post, align = "c", caption = "Pairwise Comparisons Different Reading Strategies")
```

Looking at the p-values of the pairwise comparisons, it can be noted that the there is a significant difference between the mean scores of the highlighting and passive reading strategies, as well as between the underlining and passive reading strategies. However, we still fail to reject the null hypothesis that the population mean values of the highlighting and underlining strategies are the same.

## Additional Analysis: Relationship between the Amount of Text Highlighted and Comprehension Scores

We will now conduct an additional analysis to explore the relationship between the amount of text highlighted and the comprehension scores, as initial EDA revealed that annotation by highlighting text generally led to the highest test scores in the data. This analysis will help us understand if there is a correlation between the proportion of text annotated and the comprehension scores.

We first merge the data containing the highlighted portion of text for each participant with their corresponding reading comprehension scores.

```{r}
df_highlight <- read_excel("./data/participant_responses.xls", sheet = "highlighted_portions")
kable(head(df_highlight), align = "c", caption = "First Six Rows of the Highlighted Portions Dataframe")
```

```{r}
kable(head(df), align = "c", caption = "First Six Rows of the Participant Responses Dataframe")
```

```{r}
df <- df %>% 
  select(id, highlight_score) %>% 
  left_join(df_highlight, by = "id")
kable(head(df), align = "c", caption = "First Six Rows of the Merged Dataframe")
```

Based on this data, we can visualize the relationship between the amount of text highlighted and the comprehension scores. We can also try placing visualizations of different regression models to see which one best describes the relationship.

```{r}
ggplot(df, aes(highlighted_portion, highlight_score)) +
  geom_point() +
  # Linear fit
  geom_smooth(aes(colour = "Linear fit"), method = "lm", se = FALSE) +
  # Quadratic fit
  geom_smooth(aes(colour = "Quadratic fit"), method = "lm", formula = y ~ poly(x, 2), se = FALSE) +
  # Logarithmic fit
  geom_smooth(aes(colour = "Logarithmic fit"), method = "lm", formula = y ~ log(x), se = FALSE) + 
  labs(
    title = "Relationship between Amount of Text Highlighted and Obtained Scores", 
    x = "Proportion of Text Highlighted", y = "Comprehension Score (%)"
  ) + 
  # Supply colours *and* legend labels
  scale_colour_manual(
    name   = "Model type", 
    values = c("Linear fit" = "red", 
               "Quadratic fit" = "blue", 
               "Logarithmic fit" = "darkseagreen")
  ) + 
  ylim(0, 100) + 
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

The three fitted lines represent different regression models: linear, quadratic, and logarithmic. To check if any of these models are statistically significant, we will fit each model and extract important metrics such as R-squared, adjusted R-squared, p-value, and RMSE(Root Mean Squared Error) for comparison.

```{r}
# Fit different regression models
linear_model <- lm(highlight_score ~ highlighted_portion, data = df)
quad_model <- lm(highlight_score ~ poly(highlighted_portion, 2), data = df)
log_model <- lm(highlight_score ~ log(highlighted_portion), data = df)

# Helper function to get important metrics for model comparison
get_metrics <- function(model, label) {
  g <- glance(model)
  tibble(
    label, 
    g$r.squared, 
    g$adj.r.squared, 
    g$p.value, 
    sqrt(mean(model$residuals^2))
  )
}

# Build a table to display the metrics for all models
model_metrics <- bind_rows(
  get_metrics(linear_model, "Linear"), 
  get_metrics(quad_model, "Quadratic"), 
  get_metrics(log_model, "Logarithmic")
)

kable(model_metrics,
      col.names = c("Model of Fit", "R Squared", "Adjusted R Squared", "P-value", "RMSE"),
      align = "c",
      caption = "Metrics for the Evaluation of Different Regression Models"
)
```

The table above shows the important metrics for each regression model. The R-squared and adjusted R-squared values indicate how well the model explains the variance in the data. The p-value indicates whether the model is statistically significant, and the RMSE provides a measure of the model's prediction error.

Looking at the results, it can be noted that the quadratic model best captures the relationship between the amount of text highlighted and the test scores. While the model explains a low amount of variance (Adjusted R^2^ = 0.161), it is the only fit that is statistically significant (p-value \< 0.05) and has the lowest RMSE compared to that of the other two models. On the other hand, we fail to reject the null hypothesis of zero coefficients for the linear and logarithmic models.

A quadratic regression explained 22% of the variance in comprehension scores (R^2^ = 0.216) and provided a significantly better fit than a linear model (R^2^ = 0.004). The fitted curve peaks at 38 % text highlighted, where the predicted score is about 91 points. Linear and log-linear alternatives were non-significant (p-value \> 0.05) and yielded lower adjusted R^2^ and higher RMSE.
